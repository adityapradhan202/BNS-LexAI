{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ff2a40",
   "metadata": {},
   "source": [
    "## Evaluating the baseline RAG with 20 test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be7853",
   "metadata": {},
   "source": [
    "### 1. Baseline RAG\n",
    "**Copy pasting the code of the naive baseline RAG system here. And doing some changes!**\n",
    "\n",
    "**Conclusions from the naive basline RAG:**\n",
    "\n",
    "* The previous baseline RAG system that we coded initially had a chunk size of 200.\n",
    "* 200 is very less for legal documents. But we were just testing how it performs with 200, k = 2, for similarity search.\n",
    "* We are not moving into the **'experiments for the purpose of improvement'** section yet.\n",
    "* So let's define a new baseline.\n",
    "\n",
    "**The new baseline will have:**\n",
    "1. Chunk size = 500\n",
    "2. Retrieve top 3 documents.\n",
    "3. Similarity search\n",
    "4. Recursive character text splitter\n",
    "5. For continuous and consistent sort of relationship between chunks - Overlap size = 50\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdaeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50272d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = 'chroma_dbs/baseline_db_v2'\n",
    "ch_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text:v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40c7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def create_vector_db(\n",
    "        splitter=ch_splitter, embed_func=embeddings, db_dir=db_dir,\n",
    "        raw_data_path='./raw_data/BNS2023.pdf'):\n",
    "    \"\"\"Creates persistent vector database if it doens't exist.\"\"\"\n",
    "    if os.path.exists(db_dir):\n",
    "        print(\"Vector database already exists!\")\n",
    "    else:\n",
    "        print(\"Vector database doesn't exist.\")\n",
    "        print(\"-> Initializing vector db creation...\")\n",
    "        loader = PyPDFLoader(file_path=raw_data_path, mode='page')\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            page.page_content = clean_text(page.page_content)\n",
    "            \n",
    "        chunks = splitter.split_documents(pages)       \n",
    "        db = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=embed_func\n",
    "        )\n",
    "        print(\"-> Adding chunks to vector db...\")\n",
    "        db.add_documents(chunks)\n",
    "        print(\"-> Chunks have been added to to vector db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7f69ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database already exists!\n"
     ]
    }
   ],
   "source": [
    "create_vector_db() # Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a22a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The retriever\n",
    "def retrieve_docs(query, db_dir, embed_func=embeddings):\n",
    "    \"\"\"Retrieves chunks and combines them into single string!\"\"\"\n",
    "\n",
    "    info_fetched = \"\"\n",
    "    db = Chroma(\n",
    "        persist_directory=db_dir, embedding_function=embed_func\n",
    "    )\n",
    "    docs = db.similarity_search(query=query, k=3)\n",
    "    for doc in docs:\n",
    "        info_fetched += doc.page_content\n",
    "\n",
    "    return info_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e598d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = retrieve_docs(\n",
    "    query=\"What is considered counterfeiting of coins, Government stamps, or currency notes, and what is the punishment?\",\n",
    "    db_dir=db_dir,\n",
    "    embed_func=embeddings\n",
    ")\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbdefdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coin, Government stamps, currency -notes or bank - notes. 178. Whoever counterfeits, or knowingly performs any part of the process of counterfeiting, any coin, stamp issued by Government for the purpose of revenue, currency -note or bank -note, shall be punished with imprisonment for life, or with imprisonment of either description for a term which may extend to ten years, and shall also be liable to fine. Explanation.—For the purposes of this Chapter,— (1) the expression “bank -note” means aUsing as genuine, forged or counte rfeit coin, Government stamp, currency-notes or bank-notes. 179. Whoever imports or exports, or sells or delivers to, or buys or receives from, any other person, or otherwise traffics or uses as genuine, any forged or counterfeit coin, stamp, currency-note or bank-note, knowing or having reason to believe the same to be forged or counterfeit, shall be punished with imprisonment for life, or with imprisonment of either description for a term which may extend towith imprisonment of either description for a term which may extend to seven years, or with fine, or with both. Explanation.—If a person establishes the possession of the forged or counterfeit coin, stamp, currency -note or bank -note to be from a lawful source, it shall not constitute an offence under this section. Making or possessing instruments or materials for forging or counterfeiting coin, Government st amp, currency -notes or bank - notes. 181. Whoever makes or mends, or performs any'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa894f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model='gemma3:4b', temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eec40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_generate(query, model):\n",
    "    context = retrieve_docs(query=query, db_dir=db_dir, embed_func=embeddings)\n",
    "    prompt_t = ChatPromptTemplate.from_messages(\n",
    "        messages=[\n",
    "            (\"system\", \"You are a helpful assistant who uses BNS(Bhartiya Nyaay Sanhita). Use the provided context to answer user query. BNS Context: {context}\"),\n",
    "            (\"system\", \"You can reply within 150 words\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt_t | model | StrOutputParser()\n",
    "    res = chain.invoke({'query':query, 'context':context})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d24b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating...\n",
      "\n",
      "According to the Bharatiya Nyaya Sanhita (BNS), counterfeiting encompasses several actions related to coins, Government stamps, and currency notes (including bank notes).\n",
      "\n",
      "**Counterfeiting Defined:**\n",
      "\n",
      "*   **Coin:** Counterfeiting a coin means creating a fake coin or performing any part of the process to do so.\n",
      "*   **Government Stamps/Currency Notes/Bank Notes:** This includes forging, creating fake versions, or knowingly participating in the process of creating a fake version of any Government stamp, currency note, or bank note.\n",
      "\n",
      "**Punishment:**\n",
      "\n",
      "The punishment varies depending on the offense:\n",
      "\n",
      "*   **178 (Counterfeiting):** Imprisonment for life or up to 10 years with a fine.\n",
      "*   **179 (Trading in Forged Items):** Imprisonment for life or up to 7 years with a fine, or both.\n",
      "*   **181 (Making/Mending Instruments):** This section deals with creating or repairing tools used for forgery, and the punishment applies if this is done with the intent to forge.\n",
      "\n",
      "**Important Note:** If a person can prove they possess a forged or counterfeit item from a lawful source, it does not constitute an offense under these sections.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating...\\n\")\n",
    "res = augment_generate(\n",
    "    query='What is considered counterfeiting of coins, Government stamps, or currency notes, and what is the punishment?',\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cb57d",
   "metadata": {},
   "source": [
    "### 2. Evaluation of Baseline RAG.\n",
    "\n",
    "To fix the asychronous TLE error of deepeval, we used synchronous configurations. But it still required batching because of TLE. The errors are not because of deepeval, these TLE errors because of the speed of local LLMs.\n",
    "\n",
    "But now, we are able to evaluate much better.\n",
    "\n",
    "**But it's very slow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e04cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import OllamaModel\n",
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualRelevancyMetric\n",
    "from deepeval.metrics import FaithfulnessMetric, ContextualPrecisionMetric, ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.evaluate import AsyncConfig\n",
    "from evaluation_dataset import questions, expected_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b92b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting AsyncConfig to false\n",
    "# Running synchronously\n",
    "async_config = AsyncConfig(run_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5bab1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering context and generating answer for question - 1\n",
      "Gathering context and generating answer for question - 2\n",
      "Gathering context and generating answer for question - 3\n",
      "Gathering context and generating answer for question - 4\n",
      "Gathering context and generating answer for question - 5\n",
      "Gathering context and generating answer for question - 6\n",
      "Gathering context and generating answer for question - 7\n",
      "Gathering context and generating answer for question - 8\n",
      "Gathering context and generating answer for question - 9\n",
      "Gathering context and generating answer for question - 10\n",
      "Gathering context and generating answer for question - 11\n",
      "Gathering context and generating answer for question - 12\n",
      "Gathering context and generating answer for question - 13\n",
      "Gathering context and generating answer for question - 14\n",
      "Gathering context and generating answer for question - 15\n",
      "Gathering context and generating answer for question - 16\n",
      "Gathering context and generating answer for question - 17\n",
      "Gathering context and generating answer for question - 18\n",
      "Gathering context and generating answer for question - 19\n",
      "Gathering context and generating answer for question - 20\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "for ind, que in enumerate(questions):\n",
    "    print(f\"Gathering context and generating answer for question - {ind + 1}\")\n",
    "    cont = retrieve_docs(query=que, db_dir=db_dir, embed_func=embeddings)\n",
    "    ans = augment_generate(query=que, model=model)\n",
    "    contexts.append(cont)\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2085687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test cases:  20\n"
     ]
    }
   ],
   "source": [
    "test_cases = []\n",
    "for i in range(len(questions)):\n",
    "    test_case = LLMTestCase(\n",
    "        input=questions[i],\n",
    "        actual_output=answers[i],\n",
    "        retrieval_context=[contexts[i]],\n",
    "        expected_output=expected_answers[i]\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "print(f\"Number of test cases:  {len(test_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422055e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = OllamaModel(\n",
    "    model='mistral',\n",
    "    base_url='http://localhost:11434',\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6720ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy = AnswerRelevancyMetric(threshold=0.5, model=judge_model)\n",
    "contextual_relevancy = ContextualRelevancyMetric(threshold=0.5, model=judge_model)\n",
    "contextual_precision = ContextualPrecisionMetric(threshold=0.5, model=judge_model)\n",
    "contextual_recall = ContextualRecallMetric(threshold=0.5, model=judge_model)\n",
    "faithfulness = FaithfulnessMetric(threshold=0.5, model=judge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b533a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of usual batches:  4\n",
      "Length of small batches: 2\n"
     ]
    }
   ],
   "source": [
    "# Batches\n",
    "batch1 = test_cases[0:4]\n",
    "batch2 = test_cases[4:8]\n",
    "batch3 = test_cases[8:12]\n",
    "batch4 = test_cases[12:16]\n",
    "batch5 = test_cases[16:20]\n",
    "print(f\"Length of usual batches:  {len(batch5)}\")\n",
    "\n",
    "# Smaller batches in case deepeval timeout (Incase LLM is taking too much time)\n",
    "# Use smaller batches of timeout error is not resolved with usual batches\n",
    "sm_batch1 = test_cases[0:2]\n",
    "sm_batch2 = test_cases[2:4]\n",
    "sm_batch3 = test_cases[4:6]\n",
    "sm_batch4 = test_cases[6:8]\n",
    "sm_batch5 = test_cases[8:10]\n",
    "sm_batch6 = test_cases[10:12]\n",
    "sm_batch7 = test_cases[12:14]\n",
    "sm_batch8 = test_cases[14:16]\n",
    "sm_batch9 = test_cases[16:18]\n",
    "sm_batch10 = test_cases[18:20]\n",
    "print(f\"Length of small batches: {len(sm_batch10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute these function calls in separate cells\n",
    "# The outputs of these function calls have been cleared for code readability\n",
    "evaluate(test_cases=batch1, metrics=[answer_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch2, metrics=[answer_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch3, metrics=[answer_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch4, metrics=[answer_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch5, metrics=[answer_relevancy], async_config=async_config)\n",
    "\n",
    "evaluate(test_cases=batch1, metrics=[contextual_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch2, metrics=[contextual_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch3, metrics=[contextual_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch4, metrics=[contextual_relevancy], async_config=async_config)\n",
    "evaluate(test_cases=batch5, metrics=[contextual_relevancy], async_config=async_config)\n",
    "\n",
    "evaluate(test_cases=batch1, metrics=[contextual_precision], async_config=async_config)\n",
    "evaluate(test_cases=batch2, metrics=[contextual_precision], async_config=async_config)\n",
    "evaluate(test_cases=batch3, metrics=[contextual_precision], async_config=async_config)\n",
    "evaluate(test_cases=batch4, metrics=[contextual_precision], async_config=async_config)\n",
    "evaluate(test_cases=batch5, metrics=[contextual_precision], async_config=async_config)\n",
    "\n",
    "evaluate(test_cases=batch1, metrics=[contextual_recall], async_config=async_config)\n",
    "evaluate(test_cases=batch2, metrics=[contextual_recall], async_config=async_config)\n",
    "evaluate(test_cases=batch3, metrics=[contextual_recall], async_config=async_config)\n",
    "evaluate(test_cases=batch4, metrics=[contextual_recall], async_config=async_config)\n",
    "\n",
    "# Only batch 5 was giving TLE\n",
    "# So let's batch the batch 5 further\n",
    "batch_5_b1 = batch5[:2]\n",
    "batch_5_b2 = batch5[2:]\n",
    "evaluate(test_cases=batch_5_b1, metrics=[contextual_recall], async_config=async_config)\n",
    "evaluate(test_cases=batch_5_b2, metrics=[contextual_recall], async_config=async_config)\n",
    "\n",
    "evaluate(test_cases=batch1, metrics=[faithfulness], async_config=async_config)\n",
    "evaluate(test_cases=batch2, metrics=[faithfulness], async_config=async_config)\n",
    "evaluate(test_cases=batch3, metrics=[faithfulness], async_config=async_config)\n",
    "evaluate(test_cases=batch4, metrics=[faithfulness], async_config=async_config)\n",
    "evaluate(test_cases=batch5, metrics=[faithfulness], async_config=async_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f47172",
   "metadata": {},
   "source": [
    "### 3. Logging the results and conclusions.\n",
    "\n",
    "Note: The scores are in the form of percentage representing the passing rates according to the threshold.\n",
    "\n",
    "**We can clearly see that there's some improvement with bigger chunk size and bigger k value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ad4ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59c9d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added succesfully!\n"
     ]
    }
   ],
   "source": [
    "Utility.log_experiment(\n",
    "    id='basline-RAG',\n",
    "    path='../logs/log.json',\n",
    "    faithfulness=85.0,\n",
    "    contextual_relevance=70.0,\n",
    "    answer_relevance=100.0,\n",
    "    contextual_precision=90.0,\n",
    "    contextual_recall=95.0,\n",
    "    commit_message=\"Evaluation of baseline RAG with 20 test cases\",\n",
    "    description='chunk-size:500, chunk-overlap:50, splitter:recursive char text, search-type:similarity with k=3, reranker:false, metadata-filtering:false, test-samples:20, rag-llm:Ollama-gemma3:4b, judge-llm:Ollama-mistral-7b, eval_tool:deep-eval'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb_venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
