{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ceb47cb",
   "metadata": {},
   "source": [
    "## Using MMR\n",
    "\n",
    "Considering the the performance of naive baseline RAG and the baseline RAG, we can conclude that the RAG system is performing really well.\n",
    "\n",
    "There is no need to perform **metadata filtering.**  \n",
    "But we can do **one last thing** before finalizing, we can try **maximum marginal relevance search(mmr)** which is sort of weak re-ranker. We won't change anything except the search type. And then we will evaluate and see if the contextual relevancy goes up by considerable amount.\n",
    "\n",
    "So we will use the same vector database that we used in baseline RAG, same chunk size, same overlap, same number of top-k documents, but mmr seach.\n",
    "\n",
    "**Instead of deepeval we will custom evaluation**. Because deepval takes a lot of time. We have to find which retrieval technique is better, and for we can code use our custom rag evaluation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e4efb",
   "metadata": {},
   "source": [
    "### 1. RAG with MMR and Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49227724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0c6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = 'chroma_dbs/baseline_db_v2'\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text:v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0890002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever for similarity search\n",
    "def retrieve_docs_similarity(query, db_dir, embed_func=embeddings):\n",
    "    \"\"\"Retrieves chunks and combines them into single string!\"\"\"\n",
    "\n",
    "    info_fetched = \"\"\n",
    "    db = Chroma(\n",
    "        persist_directory=db_dir, embedding_function=embed_func\n",
    "    )\n",
    "    # Maximum marginal relevance search (MMR)\n",
    "    docs = db.similarity_search(query=query, k=3)\n",
    "    for doc in docs:\n",
    "        info_fetched += doc.page_content\n",
    "\n",
    "    return info_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d8dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model='gemma3:4b', temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3331907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for augmenting and generating with similarity search\n",
    "def augment_generate_similarity(query, model):\n",
    "    context = retrieve_docs_similarity(query=query, db_dir=db_dir, embed_func=embeddings)\n",
    "    prompt_t = ChatPromptTemplate.from_messages(\n",
    "        messages=[\n",
    "            (\"system\", \"You are a helpful assistant who uses BNS(Bhartiya Nyaay Sanhita). Use the provided context to answer user query. BNS Context: {context}\"),\n",
    "            (\"system\", \"You can reply within 150 words\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt_t | model | StrOutputParser()\n",
    "    res = chain.invoke({'query':query, 'context':context})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14dab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_docs = retrieve_docs_similarity(\n",
    "    db_dir=db_dir,\n",
    "    embed_func=embeddings,\n",
    "    query='What is considered counterfeiting of coins, Government stamps, or currency notes, and what is the punishment?'\n",
    ")\n",
    "\n",
    "len(fetched_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coin, Government stamps, currency -notes or bank - notes. 178. Whoever counterfeits, or knowingly performs any part of the process of counterfeiting, any coin, stamp issued by Government for the purpose of revenue, currency -note or bank -note, shall be punished with imprisonment for life, or with imprisonment of either description for a term which may extend to ten years, and shall also be liable to fine. Explanation.—For the purposes of this Chapter,— (1) the expression “bank -note” means aUsing as genuine, forged or counte rfeit coin, Government stamp, currency-notes or bank-notes. 179. Whoever imports or exports, or sells or delivers to, or buys or receives from, any other person, or otherwise traffics or uses as genuine, any forged or counterfeit coin, stamp, currency-note or bank-note, knowing or having reason to believe the same to be forged or counterfeit, shall be punished with imprisonment for life, or with imprisonment of either description for a term which may extend towith imprisonment of either description for a term which may extend to seven years, or with fine, or with both. Explanation.—If a person establishes the possession of the forged or counterfeit coin, stamp, currency -note or bank -note to be from a lawful source, it shall not constitute an offence under this section. Making or possessing instruments or materials for forging or counterfeiting coin, Government st amp, currency -notes or bank - notes. 181. Whoever makes or mends, or performs any'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c39ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating...\n",
      "\n",
      "According to the Bharatiya Nyay Sanhita (BNS), counterfeiting encompasses several actions related to coins, Government stamps, and currency notes (including bank notes).\n",
      "\n",
      "**Counterfeiting Defined:**\n",
      "\n",
      "*   **Coin:** Counterfeiting a coin means creating a fake coin or performing any part of the process to do so.\n",
      "*   **Government Stamps/Currency Notes/Bank Notes:** This includes forging, creating fake versions of, or knowingly using any Government stamp, currency note, or bank note as if it were genuine.\n",
      "\n",
      "**Punishment:**\n",
      "\n",
      "The punishment varies depending on the offense:\n",
      "\n",
      "*   **178 (Counterfeiting):** Imprisonment for life, or up to 10 years imprisonment and a fine.\n",
      "*   **179 (Trading in Forged/Counterfeit Items):** Imprisonment for life, or up to 7 years imprisonment, a fine, or both.\n",
      "*   **181 (Making/Mending Instruments):** Making or mending instruments for forging or counterfeiting carries a punishment, though the specific details aren't provided in the excerpt.\n",
      "\n",
      "**Important Note:** If a person can prove the forged or counterfeit item came from a lawful source, it does not constitute an offense under these sections.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating...\\n\")\n",
    "res = augment_generate_similarity(\n",
    "    query='What is considered counterfeiting of coins, Government stamps, or currency notes, and what is the punishment?',\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8226a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever for similarity search\n",
    "def retrieve_docs_mmr(query, db_dir, embed_func=embeddings):\n",
    "    \"\"\"Retrieves chunks and combines them into single string!\"\"\"\n",
    "\n",
    "    info_fetched = \"\"\n",
    "    db = Chroma(\n",
    "        persist_directory=db_dir, embedding_function=embed_func\n",
    "    )\n",
    "    # Maximum marginal relevance search (MMR)\n",
    "    docs = db.max_marginal_relevance_search(query=query, lambda_mult=0.6, fetch_k=12, k=3)\n",
    "    for doc in docs:\n",
    "        info_fetched += doc.page_content\n",
    "\n",
    "    return info_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for augmenting and generating with maximum relevance search\n",
    "def augment_generate_mmr(query, model):\n",
    "    context = retrieve_docs_mmr(query=query, db_dir=db_dir, embed_func=embeddings)\n",
    "    prompt_t = ChatPromptTemplate.from_messages(\n",
    "        messages=[\n",
    "            (\"system\", \"You are a helpful assistant who uses BNS(Bhartiya Nyaay Sanhita). Use the provided context to answer user query. BNS Context: {context}\"),\n",
    "            (\"system\", \"You can reply within 150 words\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt_t | model | StrOutputParser()\n",
    "    res = chain.invoke({'query':query, 'context':context})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b9e9f",
   "metadata": {},
   "source": [
    "### 2. Using custom evaluation to find out better retrieval technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_eval import RetrievalSearchComparision\n",
    "from evaluation_dataset import questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_1 = []\n",
    "context_2 = []\n",
    "for i in range(len(questions)):\n",
    "    cnt_1 = retrieve_docs_similarity(query=questions[i], db_dir=db_dir, embed_func=embeddings)\n",
    "    cnt_2 = retrieve_docs_mmr(query=questions[i], db_dir=db_dir, embed_func=embeddings)\n",
    "    context_1.append(cnt_1)\n",
    "    context_2.append(cnt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RetrievalSearchComparision(\n",
    "    queries=questions,\n",
    "    context_type_1=context_1,\n",
    "    context_type_2=context_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f819b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating question number - 1\n",
      "Evaluating question number - 2\n",
      "Evaluating question number - 3\n",
      "Evaluating question number - 4\n",
      "Evaluating question number - 5\n",
      "Evaluating question number - 6\n",
      "Evaluating question number - 7\n",
      "Evaluating question number - 8\n",
      "Evaluating question number - 9\n",
      "Evaluating question number - 10\n",
      "Evaluating question number - 11\n",
      "Evaluating question number - 12\n",
      "Evaluating question number - 13\n",
      "Evaluating question number - 14\n",
      "Evaluating question number - 15\n",
      "Evaluating question number - 16\n",
      "Evaluating question number - 17\n",
      "Evaluating question number - 18\n",
      "Evaluating question number - 19\n",
      "Evaluating question number - 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'retrieval_type_1': 90.0, 'retrieval_type_2': 10.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_llm = ChatOllama(model='mistral', temperature=0.0)\n",
    "res = dataset.compare_contexts(\n",
    "    model=judge_llm\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264aac1d",
   "metadata": {},
   "source": [
    "### 3. Log the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aede0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added succesfully!\n"
     ]
    }
   ],
   "source": [
    "Utility.log_custom_experiment(\n",
    "    id='baseline-vs-reranking',\n",
    "    path='../logs/log.json',\n",
    "    log_data={\n",
    "        \"eval-score\":res,\n",
    "        \"eval-score-description\":\"Percentage of relevant context from similarity search is 90 percent. And it's 10 percent for mmr. So even weak re-ranking is making it worse.\",\n",
    "        \"log-commit-message\":\"Comparing similarity search and mmr\",\n",
    "        \"log-description\":\"chunk-size:500, chunk-overlap:50, splitter:recursive char text, search-type:similarity/mmr(fetch_k=12) with k=3, reranker:True(mmr), metadata-filtering:false, number of queries and context:50, rag-llm:Ollama-gemma3:4b, judge-llm:Ollama-mistral-7b, eval_tool:custom_eval\",\n",
    "        \"date\": {\n",
    "            \"day\": 30,\n",
    "            \"month\": 1,\n",
    "            \"year\": 2026\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969851d",
   "metadata": {},
   "source": [
    "**Conclusion:** So we won't use a reranker and we will go with simple similarity search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb_venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
